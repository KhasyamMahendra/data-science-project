import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier  # Example model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Data path (replace with your actual data path)
data_path = "your_transaction_data.csv"

# Read the transaction data
df = pd.read_csv(data_path)

# Data cleaning and preprocessing (replace with your specific logic)
# - Handle missing values (e.g., impute with median or domain-specific strategies)
# - Address outliers (e.g., winsorize or cap values)
# - Encode categorical features (e.g., one-hot encoding)
# - Create new features (e.g., transaction velocity, location checks)

# Separate features (X) and target variable (y)
X = df.drop('is_fraudulent', axis=1)  # Replace 'is_fraudulent' with your fraud label column
y = df['is_fraudulent']

# Balance the dataset if fraudulent transactions are rare (optional)
# Techniques like oversampling or SMOTE can be used

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest model (consider experimenting with other algorithms)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Further improvements (consider incorporating):
# - Hyperparameter tuning to optimize model performance
# - Feature importance analysis to understand key factors for fraud detection
# - Cost-sensitive learning if false positives are more costly
# - Model explainability techniques (e.g., LIME) for transparency

